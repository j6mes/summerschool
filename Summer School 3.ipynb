{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b4acf759",
   "metadata": {},
   "source": [
    "# Knowledge Intensive NLP Summer School\n",
    "\n",
    "##Â Notebook 3\n",
    "\n",
    "The goals of this notebook are:\n",
    "\n",
    "* Train a T5 notebook for SQuAD task\n",
    "* Evaluate what happens when context isn't used\n",
    "* Explore the Fusion in Decoder model \n",
    "\n",
    "\n",
    "## Resources\n",
    "You can find help for the HuggingFace library from their website: \n",
    "\n",
    "* T5 https://huggingface.co/docs/transformers/model_doc/t5\n",
    "* Datasets https://huggingface.co/docs/datasets/index\n",
    "\n",
    "## Tutorial\n",
    "\n",
    "This notebook is based on the following tutorials:\n",
    "\n",
    "* Fine-tuning https://huggingface.co/docs/transformers/training\n",
    "* Language Generation https://ai.googleblog.com/2020/02/exploring-transfer-learning-with-t5.html\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b2cadca",
   "metadata": {},
   "source": [
    "# Prelude\n",
    "\n",
    "The following code will use the SQuAD dataset and train a model to predict an answer given a question and passage. Have a look at it and familiarize yourself"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f00aec96",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: xfact in ./miniconda3/envs/summerschool/lib/python3.8/site-packages (0.3.56)\n",
      "Requirement already satisfied: urllib3==1.26.12 in ./miniconda3/envs/summerschool/lib/python3.8/site-packages (from xfact) (1.26.12)\n",
      "Requirement already satisfied: requests==2.28.1 in ./miniconda3/envs/summerschool/lib/python3.8/site-packages (from xfact) (2.28.1)\n",
      "Requirement already satisfied: transformers in ./miniconda3/envs/summerschool/lib/python3.8/site-packages (from xfact) (4.30.2)\n",
      "Requirement already satisfied: fever-drqa==1.0.13 in ./miniconda3/envs/summerschool/lib/python3.8/site-packages (from xfact) (1.0.13)\n",
      "Requirement already satisfied: scikit-learn==1.1.3 in ./miniconda3/envs/summerschool/lib/python3.8/site-packages (from xfact) (1.1.3)\n",
      "Requirement already satisfied: comet-ml==3.31.21 in ./miniconda3/envs/summerschool/lib/python3.8/site-packages (from xfact) (3.31.21)\n",
      "Requirement already satisfied: overrides==7.3.1 in ./miniconda3/envs/summerschool/lib/python3.8/site-packages (from xfact) (7.3.1)\n",
      "Requirement already satisfied: jsonschema!=3.1.0,>=2.6.0 in ./miniconda3/envs/summerschool/lib/python3.8/site-packages (from comet-ml==3.31.21->xfact) (4.17.3)\n",
      "Requirement already satisfied: requests-toolbelt>=0.8.0 in ./miniconda3/envs/summerschool/lib/python3.8/site-packages (from comet-ml==3.31.21->xfact) (1.0.0)\n",
      "Requirement already satisfied: semantic-version>=2.8.0 in ./miniconda3/envs/summerschool/lib/python3.8/site-packages (from comet-ml==3.31.21->xfact) (2.10.0)\n",
      "Requirement already satisfied: sentry-sdk>=1.1.0 in ./miniconda3/envs/summerschool/lib/python3.8/site-packages (from comet-ml==3.31.21->xfact) (1.27.0)\n",
      "Requirement already satisfied: simplejson in ./miniconda3/envs/summerschool/lib/python3.8/site-packages (from comet-ml==3.31.21->xfact) (3.19.1)\n",
      "Requirement already satisfied: six in ./miniconda3/envs/summerschool/lib/python3.8/site-packages (from comet-ml==3.31.21->xfact) (1.16.0)\n",
      "Requirement already satisfied: websocket-client<1.4.0,>=0.55.0 in ./miniconda3/envs/summerschool/lib/python3.8/site-packages (from comet-ml==3.31.21->xfact) (1.3.3)\n",
      "Requirement already satisfied: wrapt>=1.11.2 in ./miniconda3/envs/summerschool/lib/python3.8/site-packages (from comet-ml==3.31.21->xfact) (1.15.0)\n",
      "Requirement already satisfied: wurlitzer>=1.0.2 in ./miniconda3/envs/summerschool/lib/python3.8/site-packages (from comet-ml==3.31.21->xfact) (3.0.3)\n",
      "Requirement already satisfied: everett[ini]>=1.0.1 in ./miniconda3/envs/summerschool/lib/python3.8/site-packages (from comet-ml==3.31.21->xfact) (3.2.0)\n",
      "Requirement already satisfied: dulwich!=0.20.33,>=0.20.6 in ./miniconda3/envs/summerschool/lib/python3.8/site-packages (from comet-ml==3.31.21->xfact) (0.21.5)\n",
      "Requirement already satisfied: numpy in ./miniconda3/envs/summerschool/lib/python3.8/site-packages (from fever-drqa==1.0.13->xfact) (1.24.4)\n",
      "Requirement already satisfied: termcolor in ./miniconda3/envs/summerschool/lib/python3.8/site-packages (from fever-drqa==1.0.13->xfact) (2.3.0)\n",
      "Requirement already satisfied: regex in ./miniconda3/envs/summerschool/lib/python3.8/site-packages (from fever-drqa==1.0.13->xfact) (2023.6.3)\n",
      "Requirement already satisfied: tqdm in ./miniconda3/envs/summerschool/lib/python3.8/site-packages (from fever-drqa==1.0.13->xfact) (4.65.0)\n",
      "Requirement already satisfied: prettytable in ./miniconda3/envs/summerschool/lib/python3.8/site-packages (from fever-drqa==1.0.13->xfact) (3.8.0)\n",
      "Requirement already satisfied: scipy in ./miniconda3/envs/summerschool/lib/python3.8/site-packages (from fever-drqa==1.0.13->xfact) (1.10.1)\n",
      "Requirement already satisfied: nltk in ./miniconda3/envs/summerschool/lib/python3.8/site-packages (from fever-drqa==1.0.13->xfact) (3.8.1)\n",
      "Requirement already satisfied: pexpect in ./miniconda3/envs/summerschool/lib/python3.8/site-packages (from fever-drqa==1.0.13->xfact) (4.8.0)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in ./miniconda3/envs/summerschool/lib/python3.8/site-packages (from requests==2.28.1->xfact) (2.1.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./miniconda3/envs/summerschool/lib/python3.8/site-packages (from requests==2.28.1->xfact) (3.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./miniconda3/envs/summerschool/lib/python3.8/site-packages (from requests==2.28.1->xfact) (2023.5.7)\n",
      "Requirement already satisfied: joblib>=1.0.0 in ./miniconda3/envs/summerschool/lib/python3.8/site-packages (from scikit-learn==1.1.3->xfact) (1.3.1)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in ./miniconda3/envs/summerschool/lib/python3.8/site-packages (from scikit-learn==1.1.3->xfact) (3.1.0)\n",
      "Requirement already satisfied: filelock in ./miniconda3/envs/summerschool/lib/python3.8/site-packages (from transformers->xfact) (3.12.2)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.14.1 in ./miniconda3/envs/summerschool/lib/python3.8/site-packages (from transformers->xfact) (0.15.1)\n",
      "Requirement already satisfied: packaging>=20.0 in ./miniconda3/envs/summerschool/lib/python3.8/site-packages (from transformers->xfact) (23.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in ./miniconda3/envs/summerschool/lib/python3.8/site-packages (from transformers->xfact) (6.0)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in ./miniconda3/envs/summerschool/lib/python3.8/site-packages (from transformers->xfact) (0.13.3)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in ./miniconda3/envs/summerschool/lib/python3.8/site-packages (from transformers->xfact) (0.3.1)\n",
      "Requirement already satisfied: configobj in ./miniconda3/envs/summerschool/lib/python3.8/site-packages (from everett[ini]>=1.0.1->comet-ml==3.31.21->xfact) (5.0.8)\n",
      "Requirement already satisfied: fsspec in ./miniconda3/envs/summerschool/lib/python3.8/site-packages (from huggingface-hub<1.0,>=0.14.1->transformers->xfact) (2023.6.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in ./miniconda3/envs/summerschool/lib/python3.8/site-packages (from huggingface-hub<1.0,>=0.14.1->transformers->xfact) (4.7.1)\n",
      "Requirement already satisfied: attrs>=17.4.0 in ./miniconda3/envs/summerschool/lib/python3.8/site-packages (from jsonschema!=3.1.0,>=2.6.0->comet-ml==3.31.21->xfact) (23.1.0)\n",
      "Requirement already satisfied: importlib-resources>=1.4.0 in ./miniconda3/envs/summerschool/lib/python3.8/site-packages (from jsonschema!=3.1.0,>=2.6.0->comet-ml==3.31.21->xfact) (5.12.0)\n",
      "Requirement already satisfied: pkgutil-resolve-name>=1.3.10 in ./miniconda3/envs/summerschool/lib/python3.8/site-packages (from jsonschema!=3.1.0,>=2.6.0->comet-ml==3.31.21->xfact) (1.3.10)\n",
      "Requirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in ./miniconda3/envs/summerschool/lib/python3.8/site-packages (from jsonschema!=3.1.0,>=2.6.0->comet-ml==3.31.21->xfact) (0.19.3)\n",
      "Requirement already satisfied: click in ./miniconda3/envs/summerschool/lib/python3.8/site-packages (from nltk->fever-drqa==1.0.13->xfact) (8.1.3)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in ./miniconda3/envs/summerschool/lib/python3.8/site-packages (from pexpect->fever-drqa==1.0.13->xfact) (0.7.0)\n",
      "Requirement already satisfied: wcwidth in ./miniconda3/envs/summerschool/lib/python3.8/site-packages (from prettytable->fever-drqa==1.0.13->xfact) (0.2.6)\n",
      "Requirement already satisfied: zipp>=3.1.0 in ./miniconda3/envs/summerschool/lib/python3.8/site-packages (from importlib-resources>=1.4.0->jsonschema!=3.1.0,>=2.6.0->comet-ml==3.31.21->xfact) (3.15.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install xfact"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fc78111e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "comet_ml is installed but `COMET_API_KEY` is not set.\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "import os\n",
    "import sys\n",
    "import transformers\n",
    "import datasets \n",
    "\n",
    "from transformers import (\n",
    "    AutoConfig,\n",
    "    AutoTokenizer,\n",
    "    HfArgumentParser,\n",
    "    Seq2SeqTrainingArguments,\n",
    "    set_seed, AutoModelForSeq2SeqLM,\n",
    "    AutoModelForSequenceClassification\n",
    ")\n",
    "from transformers import Seq2SeqTrainer\n",
    "from transformers.trainer_utils import get_last_checkpoint\n",
    "from transformers.utils import check_min_version\n",
    "from collections import defaultdict\n",
    "from operator import itemgetter\n",
    "\n",
    "from xfact.config.args import ModelArguments, DataTrainingArguments\n",
    "from xfact.logs.comet_callback import CometTrainingCallback\n",
    "from xfact.logs.logs import setup_logging\n",
    "from xfact.nlp.dataset import XFactDataset, XFactSeq2SeqDataset\n",
    "from xfact.nlp.model import ModelFactory\n",
    "from xfact.nlp.post_processing import PostProcessor\n",
    "from xfact.nlp.reader import Reader\n",
    "from xfact.nlp.scoring import Scorer\n",
    "from xfact.registry.module import import_submodules\n",
    "\n",
    "\n",
    "check_min_version(\"4.16.0\")\n",
    "logger = logging.getLogger(__name__)\n",
    "set_seed(1337)\n",
    "\n",
    "setup_logging(\"INFO\")\n",
    "transformers.utils.logging.set_verbosity(\"INFO\")\n",
    "transformers.utils.logging.enable_default_handler()\n",
    "transformers.utils.logging.enable_explicit_format()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9ccc4d67",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|training_args.py:1267] 2023-07-05 13:50:40,377 >> Found safetensors installation, but --save_safetensors=False. Safetensors should be a preferred weights saving format due to security and performance reasons. If your model cannot be saved by safetensors please feel free to open an issue at https://github.com/huggingface/safetensors!\n",
      "[INFO|training_args.py:1669] 2023-07-05 13:50:40,377 >> PyTorch: setting up devices\n",
      "[INFO|training_args.py:1407] 2023-07-05 13:50:40,436 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n"
     ]
    }
   ],
   "source": [
    "model_args = ModelArguments(\n",
    "    model_name_or_path=\"t5-small\",\n",
    ")\n",
    "data_args = DataTrainingArguments(dataset=\"squad\",train_file=\"train\", validation_file=\"validation\")\n",
    "\n",
    "\n",
    "#NOTE SET MPS to False if error in training\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    learning_rate=5e-5,\n",
    "    logging_steps=10,\n",
    "    logging_strategy=\"steps\",\n",
    "    eval_steps=10,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    do_eval=True,\n",
    "    do_train=True,\n",
    "    output_dir=\"test\",\n",
    "    use_mps_device=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "325b0ac1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|tokenization_utils_base.py:1823] 2023-07-05 13:50:40,920 >> loading file spiece.model from cache at /Users/user/.cache/huggingface/hub/models--t5-small/snapshots/df1b051c49625cf57a3d0d8d3863ed4d13564fe4/spiece.model\n",
      "[INFO|tokenization_utils_base.py:1823] 2023-07-05 13:50:40,921 >> loading file tokenizer.json from cache at /Users/user/.cache/huggingface/hub/models--t5-small/snapshots/df1b051c49625cf57a3d0d8d3863ed4d13564fe4/tokenizer.json\n",
      "[INFO|tokenization_utils_base.py:1823] 2023-07-05 13:50:40,922 >> loading file added_tokens.json from cache at None\n",
      "[INFO|tokenization_utils_base.py:1823] 2023-07-05 13:50:40,923 >> loading file special_tokens_map.json from cache at None\n",
      "[INFO|tokenization_utils_base.py:1823] 2023-07-05 13:50:40,924 >> loading file tokenizer_config.json from cache at /Users/user/.cache/huggingface/hub/models--t5-small/snapshots/df1b051c49625cf57a3d0d8d3863ed4d13564fe4/tokenizer_config.json\n",
      "[INFO|configuration_utils.py:669] 2023-07-05 13:50:41,425 >> loading configuration file config.json from cache at /Users/user/.cache/huggingface/hub/models--t5-small/snapshots/df1b051c49625cf57a3d0d8d3863ed4d13564fe4/config.json\n",
      "[INFO|configuration_utils.py:725] 2023-07-05 13:50:41,441 >> Model config T5Config {\n",
      "  \"_name_or_path\": \"t5-small\",\n",
      "  \"architectures\": [\n",
      "    \"T5ForConditionalGeneration\"\n",
      "  ],\n",
      "  \"d_ff\": 2048,\n",
      "  \"d_kv\": 64,\n",
      "  \"d_model\": 512,\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"dense_act_fn\": \"relu\",\n",
      "  \"dropout_rate\": 0.1,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"feed_forward_proj\": \"relu\",\n",
      "  \"initializer_factor\": 1.0,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"is_gated_act\": false,\n",
      "  \"layer_norm_epsilon\": 1e-06,\n",
      "  \"model_type\": \"t5\",\n",
      "  \"n_positions\": 512,\n",
      "  \"num_decoder_layers\": 6,\n",
      "  \"num_heads\": 8,\n",
      "  \"num_layers\": 6,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"relative_attention_max_distance\": 128,\n",
      "  \"relative_attention_num_buckets\": 32,\n",
      "  \"task_specific_params\": {\n",
      "    \"summarization\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"length_penalty\": 2.0,\n",
      "      \"max_length\": 200,\n",
      "      \"min_length\": 30,\n",
      "      \"no_repeat_ngram_size\": 3,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"summarize: \"\n",
      "    },\n",
      "    \"translation_en_to_de\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to German: \"\n",
      "    },\n",
      "    \"translation_en_to_fr\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to French: \"\n",
      "    },\n",
      "    \"translation_en_to_ro\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to Romanian: \"\n",
      "    }\n",
      "  },\n",
      "  \"transformers_version\": \"4.30.2\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32128\n",
      "}\n",
      "\n",
      "[INFO|modeling_utils.py:2578] 2023-07-05 13:50:41,472 >> loading weights file model.safetensors from cache at /Users/user/.cache/huggingface/hub/models--t5-small/snapshots/df1b051c49625cf57a3d0d8d3863ed4d13564fe4/model.safetensors\n",
      "[INFO|configuration_utils.py:577] 2023-07-05 13:50:41,508 >> Generate config GenerationConfig {\n",
      "  \"_from_model_config\": true,\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"transformers_version\": \"4.30.2\"\n",
      "}\n",
      "\n",
      "[INFO|modeling_utils.py:3295] 2023-07-05 13:50:42,314 >> All model checkpoint weights were used when initializing T5ForConditionalGeneration.\n",
      "\n",
      "[INFO|modeling_utils.py:3303] 2023-07-05 13:50:42,314 >> All the weights of T5ForConditionalGeneration were initialized from the model checkpoint at t5-small.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use T5ForConditionalGeneration for predictions without further training.\n",
      "[INFO|configuration_utils.py:539] 2023-07-05 13:50:42,654 >> loading configuration file generation_config.json from cache at /Users/user/.cache/huggingface/hub/models--t5-small/snapshots/df1b051c49625cf57a3d0d8d3863ed4d13564fe4/generation_config.json\n",
      "[INFO|configuration_utils.py:577] 2023-07-05 13:50:42,655 >> Generate config GenerationConfig {\n",
      "  \"_from_model_config\": true,\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"eos_token_id\": 1,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"transformers_version\": \"4.30.2\"\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Load pretrained model and tokenizer\n",
    "#\n",
    "# Distributed training:\n",
    "# The .from_pretrained methods guarantee that only one local process can concurrently\n",
    "# download model & vocab.\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    model_args.tokenizer_name if model_args.tokenizer_name else model_args.model_name_or_path,\n",
    "    cache_dir=model_args.cache_dir,\n",
    "    use_fast=True,\n",
    "    revision=model_args.model_revision,\n",
    "    use_auth_token=True if model_args.use_auth_token else None,\n",
    ")\n",
    "\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\n",
    "    model_args.model_name_or_path,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e76d2d4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-05 13:50:44,686 [\u001b[1m datasets.builder \u001b[0m][ \u001b[1;33mWARNING\u001b[0m ] \u001b[1;33mFound cached dataset squad (/Users/user/.cache/huggingface/datasets/squad/plain_text/1.0.0/d6ec3ceb99ca480ce37cdd35555d6cb2511d223b9150cce08a837ef62ffea453)\u001b[0m (builder.py:835)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ba7efc347bd14bdea67f7b51a5a9fea7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load the dataset, we are only using 1000 training examples and 100 test for now\n",
    "squad_dataset = datasets.load_dataset(data_args.dataset, split=[\"train[:1000]\",\"validation[:100]\"])\n",
    "squad_dataset = {\n",
    "    \"train\":squad_dataset[0],\n",
    "    \"validation\":squad_dataset[1],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a8ab41f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': '5733be284776f41900661182',\n",
       " 'title': 'University_of_Notre_Dame',\n",
       " 'context': 'Architecturally, the school has a Catholic character. Atop the Main Building\\'s gold dome is a golden statue of the Virgin Mary. Immediately in front of the Main Building and facing it, is a copper statue of Christ with arms upraised with the legend \"Venite Ad Me Omnes\". Next to the Main Building is the Basilica of the Sacred Heart. Immediately behind the basilica is the Grotto, a Marian place of prayer and reflection. It is a replica of the grotto at Lourdes, France where the Virgin Mary reputedly appeared to Saint Bernadette Soubirous in 1858. At the end of the main drive (and in a direct line that connects through 3 statues and the Gold Dome), is a simple, modern stone statue of Mary.',\n",
       " 'question': 'To whom did the Virgin Mary allegedly appear in 1858 in Lourdes France?',\n",
       " 'answers': {'text': ['Saint Bernadette Soubirous'], 'answer_start': [515]}}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "squad_dataset['train'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b7305475",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-05 13:50:44,783 [\u001b[1m xfact.nlp.dataset \u001b[0m][ \u001b[1;34mINFO\u001b[0m ] \u001b[1;34mTokenizer doesn't have a sep token. Create it\u001b[0m (dataset.py:51)\n",
      "2023-07-05 13:50:44,794 [\u001b[1m xfact.nlp.dataset \u001b[0m][ \u001b[1;34mINFO\u001b[0m ] \u001b[1;34mSep token id is 32100\u001b[0m (dataset.py:57)\n",
      "train: 1000it [00:00, 21170.31it/s]\n",
      "Generating instances: 106it [00:00, 1056.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To whom did the Virgin Mary allegedly appear in 1858 in Lourdes France? - Architecturally, the school has a Catholic character. Atop the Main Building's gold dome is a golden statue of the Virgin Mary. Immediately in front of the Main Building and facing it, is a copper statue of Christ with arms upraised with the legend \"Venite Ad Me Omnes\". Next to the Main Building is the Basilica of the Sacred Heart. Immediately behind the basilica is the Grotto, a Marian place of prayer and reflection. It is a replica of the grotto at Lourdes, France where the Virgin Mary reputedly appeared to Saint Bernadette Soubirous in 1858. At the end of the main drive (and in a direct line that connects through 3 statues and the Gold Dome), is a simple, modern stone statue of Mary.\n",
      "tensor([  304,  4068,   410,     8, 16823,  3790,     3, 18280,  2385,    16,\n",
      "          507,  3449,    16,   301,  1211,  1395,  1410,    58,     3,    18,\n",
      "        30797,   120,     6,     8,   496,    65,     3,     9,  6502,  1848,\n",
      "            5,    71,  2916,     8,  5140,  5450,    31,     7,  2045, 22161,\n",
      "           19,     3,     9,  7069, 12647,    13,     8, 16823,  3790,     5,\n",
      "            3, 29167,    16,   851,    13,     8,  5140,  5450,    11,  5008,\n",
      "           34,     6,    19,     3,     9,  8658, 12647,    13,  2144,    28,\n",
      "         6026,     3,    76, 24266,    28,     8,  9503,    96,   553,    15,\n",
      "         7980,  1980,  1212, 13285,  1496,  1280,  3021,    12,     8,  5140,\n",
      "         5450,    19,     8, 23711,  2617,    13,     8,     3, 24756,  6219,\n",
      "            5,     3, 29167,  1187,     8, 20605,  2617,    19,     8,  8554,\n",
      "           17,   235,     6,     3,     9, 17535,   286,    13,  7029,    11,\n",
      "         9619,     5,    94,    19,     3,     9, 16455,     1])\n",
      "Saint Bernadette Soubirous\n",
      "tensor([2788, 8942,    9,   26, 1954,  264, 8371, 8283,    1,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0])\n",
      "****************************************************************************************************\n",
      "What is in front of the Notre Dame Main Building? - Architecturally, the school has a Catholic character. Atop the Main Building's gold dome is a golden statue of the Virgin Mary. Immediately in front of the Main Building and facing it, is a copper statue of Christ with arms upraised with the legend \"Venite Ad Me Omnes\". Next to the Main Building is the Basilica of the Sacred Heart. Immediately behind the basilica is the Grotto, a Marian place of prayer and reflection. It is a replica of the grotto at Lourdes, France where the Virgin Mary reputedly appeared to Saint Bernadette Soubirous in 1858. At the end of the main drive (and in a direct line that connects through 3 statues and the Gold Dome), is a simple, modern stone statue of Mary.\n",
      "tensor([  363,    19,    16,   851,    13,     8,  7711,     3, 17084,  5140,\n",
      "         5450,    58,     3,    18, 30797,   120,     6,     8,   496,    65,\n",
      "            3,     9,  6502,  1848,     5,    71,  2916,     8,  5140,  5450,\n",
      "           31,     7,  2045, 22161,    19,     3,     9,  7069, 12647,    13,\n",
      "            8, 16823,  3790,     5,     3, 29167,    16,   851,    13,     8,\n",
      "         5140,  5450,    11,  5008,    34,     6,    19,     3,     9,  8658,\n",
      "        12647,    13,  2144,    28,  6026,     3,    76, 24266,    28,     8,\n",
      "         9503,    96,   553,    15,  7980,  1980,  1212, 13285,  1496,  1280,\n",
      "         3021,    12,     8,  5140,  5450,    19,     8, 23711,  2617,    13,\n",
      "            8,     3, 24756,  6219,     5,     3, 29167,  1187,     8, 20605,\n",
      "         2617,    19,     8,  8554,    17,   235,     6,     3,     9, 17535,\n",
      "          286,    13,  7029,    11,  9619,     5,    94,    19,     3,     9,\n",
      "        16455,    13,     8,     3,  3844,    17,   235,     1])\n",
      "a copper statue of Christ\n",
      "tensor([    3,     9,  8658, 12647,    13,  2144,     1,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0])\n",
      "****************************************************************************************************\n",
      "The Basilica of the Sacred heart at Notre Dame is beside to which structure? - Architecturally, the school has a Catholic character. Atop the Main Building's gold dome is a golden statue of the Virgin Mary. Immediately in front of the Main Building and facing it, is a copper statue of Christ with arms upraised with the legend \"Venite Ad Me Omnes\". Next to the Main Building is the Basilica of the Sacred Heart. Immediately behind the basilica is the Grotto, a Marian place of prayer and reflection. It is a replica of the grotto at Lourdes, France where the Virgin Mary reputedly appeared to Saint Bernadette Soubirous in 1858. At the end of the main drive (and in a direct line that connects through 3 statues and the Gold Dome), is a simple, modern stone statue of Mary.\n",
      "tensor([   37, 23711,  2617,    13,     8,     3, 24756,   842,    44,  7711,\n",
      "            3, 17084,    19, 14898,    12,    84,  1809,    58,     3,    18,\n",
      "        30797,   120,     6,     8,   496,    65,     3,     9,  6502,  1848,\n",
      "            5,    71,  2916,     8,  5140,  5450,    31,     7,  2045, 22161,\n",
      "           19,     3,     9,  7069, 12647,    13,     8, 16823,  3790,     5,\n",
      "            3, 29167,    16,   851,    13,     8,  5140,  5450,    11,  5008,\n",
      "           34,     6,    19,     3,     9,  8658, 12647,    13,  2144,    28,\n",
      "         6026,     3,    76, 24266,    28,     8,  9503,    96,   553,    15,\n",
      "         7980,  1980,  1212, 13285,  1496,  1280,  3021,    12,     8,  5140,\n",
      "         5450,    19,     8, 23711,  2617,    13,     8,     3, 24756,  6219,\n",
      "            5,     3, 29167,  1187,     8, 20605,  2617,    19,     8,  8554,\n",
      "           17,   235,     6,     3,     9, 17535,   286,    13,  7029,    11,\n",
      "         9619,     5,    94,    19,     3,     9, 16455,     1])\n",
      "the Main Building\n",
      "tensor([   8, 5140, 5450,    1,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0])\n",
      "****************************************************************************************************\n",
      "What is the Grotto at Notre Dame? - Architecturally, the school has a Catholic character. Atop the Main Building's gold dome is a golden statue of the Virgin Mary. Immediately in front of the Main Building and facing it, is a copper statue of Christ with arms upraised with the legend \"Venite Ad Me Omnes\". Next to the Main Building is the Basilica of the Sacred Heart. Immediately behind the basilica is the Grotto, a Marian place of prayer and reflection. It is a replica of the grotto at Lourdes, France where the Virgin Mary reputedly appeared to Saint Bernadette Soubirous in 1858. At the end of the main drive (and in a direct line that connects through 3 statues and the Gold Dome), is a simple, modern stone statue of Mary.\n",
      "tensor([  363,    19,     8,  8554,    17,   235,    44,  7711,     3, 17084,\n",
      "           58,     3,    18, 30797,   120,     6,     8,   496,    65,     3,\n",
      "            9,  6502,  1848,     5,    71,  2916,     8,  5140,  5450,    31,\n",
      "            7,  2045, 22161,    19,     3,     9,  7069, 12647,    13,     8,\n",
      "        16823,  3790,     5,     3, 29167,    16,   851,    13,     8,  5140,\n",
      "         5450,    11,  5008,    34,     6,    19,     3,     9,  8658, 12647,\n",
      "           13,  2144,    28,  6026,     3,    76, 24266,    28,     8,  9503,\n",
      "           96,   553,    15,  7980,  1980,  1212, 13285,  1496,  1280,  3021,\n",
      "           12,     8,  5140,  5450,    19,     8, 23711,  2617,    13,     8,\n",
      "            3, 24756,  6219,     5,     3, 29167,  1187,     8, 20605,  2617,\n",
      "           19,     8,  8554,    17,   235,     6,     3,     9, 17535,   286,\n",
      "           13,  7029,    11,  9619,     5,    94,    19,     3,     9, 16455,\n",
      "           13,     8,     3,  3844,    17,   235,    44,     1])\n",
      "a Marian place of prayer and reflection\n",
      "tensor([    3,     9, 17535,   286,    13,  7029,    11,  9619,     1,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0])\n",
      "****************************************************************************************************\n",
      "What sits on top of the Main Building at Notre Dame? - Architecturally, the school has a Catholic character. Atop the Main Building's gold dome is a golden statue of the Virgin Mary. Immediately in front of the Main Building and facing it, is a copper statue of Christ with arms upraised with the legend \"Venite Ad Me Omnes\". Next to the Main Building is the Basilica of the Sacred Heart. Immediately behind the basilica is the Grotto, a Marian place of prayer and reflection. It is a replica of the grotto at Lourdes, France where the Virgin Mary reputedly appeared to Saint Bernadette Soubirous in 1858. At the end of the main drive (and in a direct line that connects through 3 statues and the Gold Dome), is a simple, modern stone statue of Mary.\n",
      "tensor([  363,  2561,     7,    30,   420,    13,     8,  5140,  5450,    44,\n",
      "         7711,     3, 17084,    58,     3,    18, 30797,   120,     6,     8,\n",
      "          496,    65,     3,     9,  6502,  1848,     5,    71,  2916,     8,\n",
      "         5140,  5450,    31,     7,  2045, 22161,    19,     3,     9,  7069,\n",
      "        12647,    13,     8, 16823,  3790,     5,     3, 29167,    16,   851,\n",
      "           13,     8,  5140,  5450,    11,  5008,    34,     6,    19,     3,\n",
      "            9,  8658, 12647,    13,  2144,    28,  6026,     3,    76, 24266,\n",
      "           28,     8,  9503,    96,   553,    15,  7980,  1980,  1212, 13285,\n",
      "         1496,  1280,  3021,    12,     8,  5140,  5450,    19,     8, 23711,\n",
      "         2617,    13,     8,     3, 24756,  6219,     5,     3, 29167,  1187,\n",
      "            8, 20605,  2617,    19,     8,  8554,    17,   235,     6,     3,\n",
      "            9, 17535,   286,    13,  7029,    11,  9619,     5,    94,    19,\n",
      "            3,     9, 16455,    13,     8,     3,  3844,     1])\n",
      "a golden statue of the Virgin Mary\n",
      "tensor([    3,     9,  7069, 12647,    13,     8, 16823,  3790,     1,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0])\n",
      "****************************************************************************************************\n",
      "When did the Scholastic Magazine of Notre dame begin publishing? - As at most other universities, Notre Dame's students run a number of news media outlets. The nine student-run outlets include three newspapers, both a radio and television station, and several magazines and journals. Begun as a one-page journal in September 1876, the Scholastic magazine is issued twice monthly and claims to be the oldest continuous collegiate publication in the United States. The other magazine, The Juggler, is released twice a year and focuses on student literature and artwork. The Dome yearbook is published annually. The newspapers have varying publication interests, with The Observer published daily and mainly reporting university and other news, and staffed by students from both Notre Dame and Saint Mary's College. Unlike Scholastic and The Dome, The Observer is an independent publication and does not have a faculty advisor or any editorial oversight from the University. In 1987, when some students believed that The Observer began to show a conservative bias, a liberal newspaper, Common Sense was published. Likewise, in 2003, when other students believed that the paper showed a liberal bias, the conservative paper Irish Rover went into production. Neither paper is published as often as The Observer; however, all three are distributed to all students. Finally, in Spring 2008 an undergraduate journal for political science research, Beyond Politics, made its debut.\n",
      "tensor([  366,   410,     8, 16064,    40, 10057,  8336,    13,  7711, 10157,\n",
      "           15,  1731,  9002,    58,     3,    18,   282,    44,   167,   119,\n",
      "         8278,     6,  7711,     3, 17084,    31,     7,   481,   661,     3,\n",
      "            9,   381,    13,  1506,   783, 14290,     5,    37,  4169,  1236,\n",
      "           18,  4312, 14290,   560,   386, 16265,     6,   321,     3,     9,\n",
      "         2252,    11,  4390,  2478,     6,    11,   633, 13254,    11, 18178,\n",
      "            5, 10129,   202,    38,     3,     9,    80,    18,  6492,  6378,\n",
      "           16,  1600,   507,  3959,     6,     8, 16064,    40, 10057,  3835,\n",
      "           19,  4683,  4394,  3718,    11,  3213,    12,    36,     8, 10043,\n",
      "         7558,     3, 31003,  5707,    16,     8,   907,  1323,     5,    37,\n",
      "          119,  3835,     6,    37,  3736,   122, 12683,     6,    19,  1883,\n",
      "         4394,     3,     9,   215,    11,     3,  6915,    30,  1236,  6678,\n",
      "           11,  7924,     5,    37, 10576,    15,   215,     1])\n",
      "September 1876\n",
      "tensor([1600,  507, 3959,    1,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0])\n",
      "****************************************************************************************************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating instances: 1000it [00:00, 1203.35it/s]\n",
      "2023-07-05 13:50:45,675 [\u001b[1m xfact.nlp.dataset \u001b[0m][ \u001b[1;34mINFO\u001b[0m ] \u001b[1;34mOutput prompt tokens are []\u001b[0m (dataset.py:164)\n",
      "2023-07-05 13:50:45,695 [\u001b[1m xfact.nlp.dataset \u001b[0m][ \u001b[1;34mINFO\u001b[0m ] \u001b[1;34mSep token id is 32100\u001b[0m (dataset.py:57)\n",
      "validation: 100it [00:00, 18351.80it/s]\n",
      "Generating instances: 100it [00:00, 1819.91it/s]\n",
      "2023-07-05 13:50:45,759 [\u001b[1m xfact.nlp.dataset \u001b[0m][ \u001b[1;34mINFO\u001b[0m ] \u001b[1;34mOutput prompt tokens are []\u001b[0m (dataset.py:164)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Which NFL team represented the AFC at Super Bowl 50? - Super Bowl 50 was an American football game to determine the champion of the National Football League (NFL) for the 2015 season. The American Football Conference (AFC) champion Denver Broncos defeated the National Football Conference (NFC) champion Carolina Panthers 24â10 to earn their third Super Bowl title. The game was played on February 7, 2016, at Levi's Stadium in the San Francisco Bay Area at Santa Clara, California. As this was the 50th Super Bowl, the league emphasized the \"golden anniversary\" with various gold-themed initiatives, as well as temporarily suspending the tradition of naming each Super Bowl game with Roman numerals (under which the game would have been known as \"Super Bowl L\"), so that the logo could prominently feature the Arabic numerals 50.\n",
      "tensor([ 4073, 10439,   372,  7283,     8,    71,  5390,    44,  2011,  9713,\n",
      "          943,    58,     3,    18,  2011,  9713,   943,    47,    46,   797,\n",
      "         3370,   467,    12,  2082,     8,  6336,    13,     8,   868, 10929,\n",
      "         3815,    41, 12619,   434,    61,    21,     8,  1230,   774,     5,\n",
      "           37,   797, 10929,  4379,    41,   188,  5390,    61,  6336, 12154,\n",
      "         4027,    29,   509,     7, 17025,     8,   868, 10929,  4379,    41,\n",
      "          567,  5390,    61,  6336,  5089, 21149,     7,   997,   104,  1714,\n",
      "           12,  3807,    70,  1025,  2011,  9713,  2233,     5,    37,   467,\n",
      "           47,  1944,    30,  2083,  7973,  5123,    44, 16755,    31,     7,\n",
      "        12750,    16,     8,  1051,  5901,  2474,  5690,    44,  4625,  9908,\n",
      "            9,     6,  1826,     5,   282,    48,    47,     8,   943,   189,\n",
      "         2011,  9713,     6,     8,  5533,     3, 25472,     8,    96, 14910,\n",
      "           35,  7685,   121,    28,   796,  2045,    18,     1])\n",
      "Denver Broncos\n",
      "tensor([12154,  4027,    29,   509,     7,     1,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0])\n",
      "****************************************************************************************************\n",
      "Which NFL team represented the NFC at Super Bowl 50? - Super Bowl 50 was an American football game to determine the champion of the National Football League (NFL) for the 2015 season. The American Football Conference (AFC) champion Denver Broncos defeated the National Football Conference (NFC) champion Carolina Panthers 24â10 to earn their third Super Bowl title. The game was played on February 7, 2016, at Levi's Stadium in the San Francisco Bay Area at Santa Clara, California. As this was the 50th Super Bowl, the league emphasized the \"golden anniversary\" with various gold-themed initiatives, as well as temporarily suspending the tradition of naming each Super Bowl game with Roman numerals (under which the game would have been known as \"Super Bowl L\"), so that the logo could prominently feature the Arabic numerals 50.\n",
      "tensor([ 4073, 10439,   372,  7283,     8,   445,  5390,    44,  2011,  9713,\n",
      "          943,    58,     3,    18,  2011,  9713,   943,    47,    46,   797,\n",
      "         3370,   467,    12,  2082,     8,  6336,    13,     8,   868, 10929,\n",
      "         3815,    41, 12619,   434,    61,    21,     8,  1230,   774,     5,\n",
      "           37,   797, 10929,  4379,    41,   188,  5390,    61,  6336, 12154,\n",
      "         4027,    29,   509,     7, 17025,     8,   868, 10929,  4379,    41,\n",
      "          567,  5390,    61,  6336,  5089, 21149,     7,   997,   104,  1714,\n",
      "           12,  3807,    70,  1025,  2011,  9713,  2233,     5,    37,   467,\n",
      "           47,  1944,    30,  2083,  7973,  5123,    44, 16755,    31,     7,\n",
      "        12750,    16,     8,  1051,  5901,  2474,  5690,    44,  4625,  9908,\n",
      "            9,     6,  1826,     5,   282,    48,    47,     8,   943,   189,\n",
      "         2011,  9713,     6,     8,  5533,     3, 25472,     8,    96, 14910,\n",
      "           35,  7685,   121,    28,   796,  2045,    18,     1])\n",
      "Carolina Panthers\n",
      "tensor([ 5089, 21149,     7,     1,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0])\n",
      "****************************************************************************************************\n",
      "Where did Super Bowl 50 take place? - Super Bowl 50 was an American football game to determine the champion of the National Football League (NFL) for the 2015 season. The American Football Conference (AFC) champion Denver Broncos defeated the National Football Conference (NFC) champion Carolina Panthers 24â10 to earn their third Super Bowl title. The game was played on February 7, 2016, at Levi's Stadium in the San Francisco Bay Area at Santa Clara, California. As this was the 50th Super Bowl, the league emphasized the \"golden anniversary\" with various gold-themed initiatives, as well as temporarily suspending the tradition of naming each Super Bowl game with Roman numerals (under which the game would have been known as \"Super Bowl L\"), so that the logo could prominently feature the Arabic numerals 50.\n",
      "tensor([ 2840,   410,  2011,  9713,   943,   240,   286,    58,     3,    18,\n",
      "         2011,  9713,   943,    47,    46,   797,  3370,   467,    12,  2082,\n",
      "            8,  6336,    13,     8,   868, 10929,  3815,    41, 12619,   434,\n",
      "           61,    21,     8,  1230,   774,     5,    37,   797, 10929,  4379,\n",
      "           41,   188,  5390,    61,  6336, 12154,  4027,    29,   509,     7,\n",
      "        17025,     8,   868, 10929,  4379,    41,   567,  5390,    61,  6336,\n",
      "         5089, 21149,     7,   997,   104,  1714,    12,  3807,    70,  1025,\n",
      "         2011,  9713,  2233,     5,    37,   467,    47,  1944,    30,  2083,\n",
      "         7973,  5123,    44, 16755,    31,     7, 12750,    16,     8,  1051,\n",
      "         5901,  2474,  5690,    44,  4625,  9908,     9,     6,  1826,     5,\n",
      "          282,    48,    47,     8,   943,   189,  2011,  9713,     6,     8,\n",
      "         5533,     3, 25472,     8,    96, 14910,    35,  7685,   121,    28,\n",
      "          796,  2045,    18, 24186,  6985,     6,    38,     1])\n",
      "Santa Clara, California\n",
      "tensor([4625, 9908,    9,    6, 1826,    1,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0])\n",
      "****************************************************************************************************\n",
      "Which NFL team won Super Bowl 50? - Super Bowl 50 was an American football game to determine the champion of the National Football League (NFL) for the 2015 season. The American Football Conference (AFC) champion Denver Broncos defeated the National Football Conference (NFC) champion Carolina Panthers 24â10 to earn their third Super Bowl title. The game was played on February 7, 2016, at Levi's Stadium in the San Francisco Bay Area at Santa Clara, California. As this was the 50th Super Bowl, the league emphasized the \"golden anniversary\" with various gold-themed initiatives, as well as temporarily suspending the tradition of naming each Super Bowl game with Roman numerals (under which the game would have been known as \"Super Bowl L\"), so that the logo could prominently feature the Arabic numerals 50.\n",
      "tensor([ 4073, 10439,   372,   751,  2011,  9713,   943,    58,     3,    18,\n",
      "         2011,  9713,   943,    47,    46,   797,  3370,   467,    12,  2082,\n",
      "            8,  6336,    13,     8,   868, 10929,  3815,    41, 12619,   434,\n",
      "           61,    21,     8,  1230,   774,     5,    37,   797, 10929,  4379,\n",
      "           41,   188,  5390,    61,  6336, 12154,  4027,    29,   509,     7,\n",
      "        17025,     8,   868, 10929,  4379,    41,   567,  5390,    61,  6336,\n",
      "         5089, 21149,     7,   997,   104,  1714,    12,  3807,    70,  1025,\n",
      "         2011,  9713,  2233,     5,    37,   467,    47,  1944,    30,  2083,\n",
      "         7973,  5123,    44, 16755,    31,     7, 12750,    16,     8,  1051,\n",
      "         5901,  2474,  5690,    44,  4625,  9908,     9,     6,  1826,     5,\n",
      "          282,    48,    47,     8,   943,   189,  2011,  9713,     6,     8,\n",
      "         5533,     3, 25472,     8,    96, 14910,    35,  7685,   121,    28,\n",
      "          796,  2045,    18, 24186,  6985,     6,    38,     1])\n",
      "Denver Broncos\n",
      "tensor([12154,  4027,    29,   509,     7,     1,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0])\n",
      "****************************************************************************************************\n",
      "What color was used to emphasize the 50th anniversary of the Super Bowl? - Super Bowl 50 was an American football game to determine the champion of the National Football League (NFL) for the 2015 season. The American Football Conference (AFC) champion Denver Broncos defeated the National Football Conference (NFC) champion Carolina Panthers 24â10 to earn their third Super Bowl title. The game was played on February 7, 2016, at Levi's Stadium in the San Francisco Bay Area at Santa Clara, California. As this was the 50th Super Bowl, the league emphasized the \"golden anniversary\" with various gold-themed initiatives, as well as temporarily suspending the tradition of naming each Super Bowl game with Roman numerals (under which the game would have been known as \"Super Bowl L\"), so that the logo could prominently feature the Arabic numerals 50.\n",
      "tensor([  363,   945,    47,   261,    12, 15523,     8,   943,   189,  7685,\n",
      "           13,     8,  2011,  9713,    58,     3,    18,  2011,  9713,   943,\n",
      "           47,    46,   797,  3370,   467,    12,  2082,     8,  6336,    13,\n",
      "            8,   868, 10929,  3815,    41, 12619,   434,    61,    21,     8,\n",
      "         1230,   774,     5,    37,   797, 10929,  4379,    41,   188,  5390,\n",
      "           61,  6336, 12154,  4027,    29,   509,     7, 17025,     8,   868,\n",
      "        10929,  4379,    41,   567,  5390,    61,  6336,  5089, 21149,     7,\n",
      "          997,   104,  1714,    12,  3807,    70,  1025,  2011,  9713,  2233,\n",
      "            5,    37,   467,    47,  1944,    30,  2083,  7973,  5123,    44,\n",
      "        16755,    31,     7, 12750,    16,     8,  1051,  5901,  2474,  5690,\n",
      "           44,  4625,  9908,     9,     6,  1826,     5,   282,    48,    47,\n",
      "            8,   943,   189,  2011,  9713,     6,     8,  5533,     3, 25472,\n",
      "            8,    96, 14910,    35,  7685,   121,    28,     1])\n",
      "gold\n",
      "tensor([2045,    1,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0])\n",
      "****************************************************************************************************\n",
      "What was the theme of Super Bowl 50? - Super Bowl 50 was an American football game to determine the champion of the National Football League (NFL) for the 2015 season. The American Football Conference (AFC) champion Denver Broncos defeated the National Football Conference (NFC) champion Carolina Panthers 24â10 to earn their third Super Bowl title. The game was played on February 7, 2016, at Levi's Stadium in the San Francisco Bay Area at Santa Clara, California. As this was the 50th Super Bowl, the league emphasized the \"golden anniversary\" with various gold-themed initiatives, as well as temporarily suspending the tradition of naming each Super Bowl game with Roman numerals (under which the game would have been known as \"Super Bowl L\"), so that the logo could prominently feature the Arabic numerals 50.\n",
      "tensor([  363,    47,     8,  3800,    13,  2011,  9713,   943,    58,     3,\n",
      "           18,  2011,  9713,   943,    47,    46,   797,  3370,   467,    12,\n",
      "         2082,     8,  6336,    13,     8,   868, 10929,  3815,    41, 12619,\n",
      "          434,    61,    21,     8,  1230,   774,     5,    37,   797, 10929,\n",
      "         4379,    41,   188,  5390,    61,  6336, 12154,  4027,    29,   509,\n",
      "            7, 17025,     8,   868, 10929,  4379,    41,   567,  5390,    61,\n",
      "         6336,  5089, 21149,     7,   997,   104,  1714,    12,  3807,    70,\n",
      "         1025,  2011,  9713,  2233,     5,    37,   467,    47,  1944,    30,\n",
      "         2083,  7973,  5123,    44, 16755,    31,     7, 12750,    16,     8,\n",
      "         1051,  5901,  2474,  5690,    44,  4625,  9908,     9,     6,  1826,\n",
      "            5,   282,    48,    47,     8,   943,   189,  2011,  9713,     6,\n",
      "            8,  5533,     3, 25472,     8,    96, 14910,    35,  7685,   121,\n",
      "           28,   796,  2045,    18, 24186,  6985,     6,     1])\n",
      "\"golden anniversary\"\n",
      "tensor([   96, 14910,    35,  7685,   121,     1,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0])\n",
      "****************************************************************************************************\n"
     ]
    }
   ],
   "source": [
    "max_seq_length = min(data_args.max_seq_length, tokenizer.model_max_length)\n",
    "\n",
    "data_files = {\n",
    "    \"train\": data_args.train_file,\n",
    "    \"validation\": data_args.validation_file\n",
    "}\n",
    "\n",
    "class SQUADWithQuestionAndContext(XFactSeq2SeqDataset):\n",
    "    def prepare_src(self, instance):\n",
    "        return instance[\"question\"] + \" - \" + instance[\"context\"]\n",
    "\n",
    "    def prepare_tgt(self, instance):\n",
    "        return instance[\"answers\"]['text'][0] or \"No Answer\"\n",
    "        \n",
    "loaded_datasets = {\n",
    "    split: SQuADPageTitlePredictionDataset(tokenizer,\n",
    "                                           squad_dataset[split],\n",
    "                                           max_seq_length,\n",
    "                                           name=split,\n",
    "                                           max_target_length=data_args.max_target_length)\n",
    "    for split, path in data_files.items()\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "13199e48",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|trainer.py:1786] 2023-07-05 13:52:05,986 >> ***** Running training *****\n",
      "[INFO|trainer.py:1787] 2023-07-05 13:52:05,986 >>   Num examples = 1,000\n",
      "[INFO|trainer.py:1788] 2023-07-05 13:52:05,986 >>   Num Epochs = 3\n",
      "[INFO|trainer.py:1789] 2023-07-05 13:52:05,987 >>   Instantaneous batch size per device = 8\n",
      "[INFO|trainer.py:1790] 2023-07-05 13:52:05,987 >>   Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "[INFO|trainer.py:1791] 2023-07-05 13:52:05,987 >>   Gradient Accumulation steps = 1\n",
      "[INFO|trainer.py:1792] 2023-07-05 13:52:05,987 >>   Total optimization steps = 375\n",
      "[INFO|trainer.py:1793] 2023-07-05 13:52:05,988 >>   Number of trainable parameters = 60,506,624\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='39' max='375' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 39/375 00:23 < 03:33, 1.57 it/s, Epoch 0.30/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Metric Name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>2.454300</td>\n",
       "      <td>1.047829</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>1.993800</td>\n",
       "      <td>0.678388</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>1.653100</td>\n",
       "      <td>0.616795</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|trainer.py:3200] 2023-07-05 13:52:10,487 >> ***** Running Evaluation *****\n",
      "[INFO|trainer.py:3202] 2023-07-05 13:52:10,487 >>   Num examples = 100\n",
      "[INFO|trainer.py:3205] 2023-07-05 13:52:10,487 >>   Batch size = 8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<transformers.trainer_utils.EvalPrediction object at 0x7fc9387371f0>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|trainer.py:3200] 2023-07-05 13:52:17,002 >> ***** Running Evaluation *****\n",
      "[INFO|trainer.py:3202] 2023-07-05 13:52:17,002 >>   Num examples = 100\n",
      "[INFO|trainer.py:3205] 2023-07-05 13:52:17,002 >>   Batch size = 8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<transformers.trainer_utils.EvalPrediction object at 0x7fc9384a7460>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|trainer.py:3200] 2023-07-05 13:52:23,458 >> ***** Running Evaluation *****\n",
      "[INFO|trainer.py:3202] 2023-07-05 13:52:23,458 >>   Num examples = 100\n",
      "[INFO|trainer.py:3205] 2023-07-05 13:52:23,459 >>   Batch size = 8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<transformers.trainer_utils.EvalPrediction object at 0x7fc9a99f1310>\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 18\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmetric_name\u001b[39m\u001b[38;5;124m\"\u001b[39m:\u001b[38;5;241m0\u001b[39m}\n\u001b[1;32m      7\u001b[0m trainer \u001b[38;5;241m=\u001b[39m Seq2SeqTrainer(\n\u001b[1;32m      8\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[1;32m      9\u001b[0m     args\u001b[38;5;241m=\u001b[39mtraining_args,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     14\u001b[0m     compute_metrics\u001b[38;5;241m=\u001b[39mmy_metrics_function,\n\u001b[1;32m     15\u001b[0m )\n\u001b[0;32m---> 18\u001b[0m train_result \u001b[38;5;241m=\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     19\u001b[0m trainer\u001b[38;5;241m.\u001b[39msave_model()\n\u001b[1;32m     21\u001b[0m metrics \u001b[38;5;241m=\u001b[39m train_result\u001b[38;5;241m.\u001b[39mmetrics\n",
      "File \u001b[0;32m~/miniconda3/envs/summerschool/lib/python3.8/site-packages/transformers/trainer.py:1645\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   1640\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_wrapped \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\n\u001b[1;32m   1642\u001b[0m inner_training_loop \u001b[38;5;241m=\u001b[39m find_executable_batch_size(\n\u001b[1;32m   1643\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_inner_training_loop, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_train_batch_size, args\u001b[38;5;241m.\u001b[39mauto_find_batch_size\n\u001b[1;32m   1644\u001b[0m )\n\u001b[0;32m-> 1645\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1646\u001b[0m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1647\u001b[0m \u001b[43m    \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1648\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1649\u001b[0m \u001b[43m    \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1650\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/summerschool/lib/python3.8/site-packages/transformers/trainer.py:1938\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   1935\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_handler\u001b[38;5;241m.\u001b[39mon_step_begin(args, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol)\n\u001b[1;32m   1937\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39maccumulate(model):\n\u001b[0;32m-> 1938\u001b[0m     tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1940\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   1941\u001b[0m     args\u001b[38;5;241m.\u001b[39mlogging_nan_inf_filter\n\u001b[1;32m   1942\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_tpu_available()\n\u001b[1;32m   1943\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39misnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch\u001b[38;5;241m.\u001b[39misinf(tr_loss_step))\n\u001b[1;32m   1944\u001b[0m ):\n\u001b[1;32m   1945\u001b[0m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[1;32m   1946\u001b[0m     tr_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m tr_loss \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_globalstep_last_logged)\n",
      "File \u001b[0;32m~/miniconda3/envs/summerschool/lib/python3.8/site-packages/transformers/trainer.py:2770\u001b[0m, in \u001b[0;36mTrainer.training_step\u001b[0;34m(self, model, inputs)\u001b[0m\n\u001b[1;32m   2768\u001b[0m         scaled_loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m   2769\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2770\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maccelerator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloss\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2772\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\u001b[38;5;241m.\u001b[39mdetach() \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mgradient_accumulation_steps\n",
      "File \u001b[0;32m~/miniconda3/envs/summerschool/lib/python3.8/site-packages/accelerate/accelerator.py:1821\u001b[0m, in \u001b[0;36mAccelerator.backward\u001b[0;34m(self, loss, **kwargs)\u001b[0m\n\u001b[1;32m   1819\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscaler\u001b[38;5;241m.\u001b[39mscale(loss)\u001b[38;5;241m.\u001b[39mbackward(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1820\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1821\u001b[0m     \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/summerschool/lib/python3.8/site-packages/torch/_tensor.py:487\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    477\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    478\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    479\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    480\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    485\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    486\u001b[0m     )\n\u001b[0;32m--> 487\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    488\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    489\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/summerschool/lib/python3.8/site-packages/torch/autograd/__init__.py:200\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    195\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    197\u001b[0m \u001b[38;5;66;03m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    198\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    199\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 200\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    201\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    202\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "data_collator = lambda batch: XFactSeq2SeqDataset.collate_fn(model, batch, tokenizer.pad_token_id, data_args.ignore_pad_token_for_loss)\n",
    "\n",
    "def my_metrics_function(eval_predictions):\n",
    "    print(str(eval_predictions))\n",
    "    return {\"metric_name\":0}\n",
    "\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=loaded_datasets[\"train\"],\n",
    "    eval_dataset=loaded_datasets[\"validation\"],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=my_metrics_function,\n",
    ")\n",
    "\n",
    "\n",
    "train_result = trainer.train()\n",
    "trainer.save_model()\n",
    "\n",
    "metrics = train_result.metrics\n",
    "trainer.log_metrics(\"train\", metrics)\n",
    "trainer.save_metrics(\"train\", metrics)\n",
    "trainer.save_state()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e93a2eb7",
   "metadata": {},
   "source": [
    "# Exercises\n",
    "\n",
    "##Â Seq2seq model T5\n",
    "1) Adapt the code to report the answer exact match accuracy (see `compute_metrics` function in HuggingFace Trainer)\n",
    "2) Compare a different version of the task, evaluating whether the model can accurately predict answers without the context paragraph. How does teh exact match answer accuracy change?\n",
    "3) Train a GENRE-style information retrieval system\n",
    "\n",
    "\n",
    "##Â Extension exercises\n",
    "\n",
    "1) Look at the Forward method in the T5Model class in HuggingFace (https://github.com/huggingface/transformers/blob/ee339bad01bf09266eba665c5f063f0ab7474dad/src/transformers/models/t5/modeling_t5.py#L1414). Explore the fusion in decoder library: https://github.com/facebookresearch/fid - How would you change this method to encode multiple passages separately?\n",
    "2) Download the FAISS library and adapt code from yesterday's lab to index the DPR-encoded contexts in FAISS and evaluate the speed-up"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Summer School",
   "language": "python",
   "name": "summerschool"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
