{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b0fcb1f1",
   "metadata": {},
   "source": [
    "# Knowledge Intensive NLP Summer School\n",
    "\n",
    "##Â Notebook 1\n",
    "\n",
    "The goals of this notebook are:\n",
    "\n",
    "* Familiarization with the Huggingface Transformers Library\n",
    "* Train a BERT model for natural language inference\n",
    "* (Extension) Train a T5 model for Question Answering\n",
    "\n",
    "\n",
    "## Resources\n",
    "You can find help for the HuggingFace library from their website: \n",
    "\n",
    "* BERT https://huggingface.co/docs/transformers/model_doc/bert\n",
    "* T5 https://huggingface.co/docs/transformers/model_doc/t5\n",
    "* Datasets https://huggingface.co/docs/datasets/index\n",
    "\n",
    "## Tutorial\n",
    "\n",
    "This notebook is based on the following tutorials:\n",
    "\n",
    "* BERT https://github.com/huggingface/transformers/tree/main/examples/pytorch/text-classification\n",
    "* Fine-tuning https://huggingface.co/docs/transformers/training\n",
    "* Language Generation https://ai.googleblog.com/2020/02/exploring-transfer-learning-with-t5.html\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07f10ef4",
   "metadata": {},
   "source": [
    "# Exercise 1 - Familiarization with BERT\n",
    "\n",
    "In this exercise, we will download a pre-trained BERT model and explore predicting missing tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7417a5ab",
   "metadata": {},
   "source": [
    "## Exercise 1.1 - Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93df9ea6",
   "metadata": {},
   "source": [
    "Download the tokenizer for BERT model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ed892a3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tok = AutoTokenizer.from_pretrained(\"bert-base-uncased\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d41abd8a",
   "metadata": {},
   "source": [
    "Tokenize a test string. Try putting in some other rarer words. What do you notice?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfcf4919",
   "metadata": {},
   "outputs": [],
   "source": [
    "tok.tokenize(\"hello world this is a test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72742eb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "tok.tokenize(\"i ate fettuccine alfredo\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8a8f83a",
   "metadata": {},
   "source": [
    "**Question: Describe the effect of the bert-base-uncased tokenizer on long / rare words**\n",
    "\n",
    "**Answer:** to be completed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27c89db9",
   "metadata": {},
   "source": [
    "The tokenizer function can also prepare the input features for the model. Use the tokenizer to \"encode\" the string in the previous example using the `tok.encode` function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dadcc379",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d3035666",
   "metadata": {},
   "source": [
    "**Question: Describe the effect of the tok.encode function in the bert-base-uncased tokenizer. Do you have more tokens than the output of tok.tokenize()? Focus on the start/end symbols of the string.**\n",
    "\n",
    "**Answer:** to be completed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a63c18cd",
   "metadata": {},
   "source": [
    "**Question: What is the data type of the encoded string after using the tok.encode function?**\n",
    "\n",
    "**Answer:** to be completed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7457a589",
   "metadata": {},
   "source": [
    "We can decode the output of the encoded tokens using the `tok.decode` function. What is the effect of the "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16a921fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# encode the string \n",
    "# decode the list of IDs\n",
    "\n",
    "print(encoded)\n",
    "print(decoded)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7118d289",
   "metadata": {},
   "source": [
    "## Exercise 1.2 - Encoding with BERT Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66e3cf72",
   "metadata": {},
   "source": [
    "Download and load the pre-trained BERT model from HuggingFace. This may take a while (440MB download)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce0b7e60",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModel\n",
    "model = AutoModel.from_pretrained(\"bert-base-uncased\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f249ff5",
   "metadata": {},
   "source": [
    "**Question** Use code to count how many parameters the BERT model has.\n",
    "\n",
    "**Answer** to be completed (hint: https://github.com/huggingface/transformers/issues/1479)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deff616c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "85bf4880",
   "metadata": {},
   "source": [
    "We can encode a string with the tokenizer and encode this with the BERT model. This assumes that `encoded` was set in Exercise 1.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f183a082",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "outputs = model(input_ids=torch.LongTensor([encoded]))\n",
    "print(outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4841f8f6",
   "metadata": {},
   "source": [
    "**Question:** When calling the model, we made 2 changes to the encoded string. What is the effect of this? Why do we use square brackets, what is the purpose of torch.LongTensor?\n",
    "\n",
    "`torch.LongTensor([encoded])`\n",
    "\n",
    "**Answer:** to be completed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47aa1b65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hint: https://pytorch.org/docs/stable/tensors.html\n",
    "\n",
    "print(torch.LongTensor([1,2,3,4]))\n",
    "print(torch.LongTensor([[1,2,3,4]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4361c77b",
   "metadata": {},
   "source": [
    "We can understand what the outputs of the model are by using the `.shape` property of a torch tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f59f5b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs.last_hidden_state.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfadbd33",
   "metadata": {},
   "source": [
    "**Question:** What do these dimensions of the 3D tensor generated by the mode correspond to? Try changing the input string or using tokenizer to encode multiple strings. (hint, you may have to encode with `padding=True` enabled)\n",
    "\n",
    "**Answer:** to be completed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c68a6bc",
   "metadata": {},
   "source": [
    "**Question:** How could this model output be used for (a) a masked language modelling task and (b) a sequence classification task.\n",
    "\n",
    "**Answer:** to be completed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeca36fc",
   "metadata": {},
   "source": [
    "## Exercise 1.3 - Language Modelling with BERT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6055fe5e",
   "metadata": {},
   "source": [
    "Load a version of the model designed for masked language modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1287f484",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForMaskedLM\n",
    "language_model = AutoModelForMaskedLM.from_pretrained(\"bert-base-uncased\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "909f926d",
   "metadata": {},
   "source": [
    "Using the test string, encode it, and make a prediction with the model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ade88d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_string = \"The capital city of Italy is [MASK].\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c4f5853",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Answer\n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "139f9de3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(outputs.logits)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2aaa02de",
   "metadata": {},
   "source": [
    "Print the `outputs.logits` shape. What do these dimensions correspond to?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82cb496a",
   "metadata": {},
   "source": [
    "**Question:** What token ID is the [MASK] token encoded as?\n",
    "\n",
    "**Answer:** to be completed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32e42b89",
   "metadata": {},
   "source": [
    "**Question:** What do the dimensions of outputs.logits correspond to?\n",
    "\n",
    "**Answer:** to be completed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c07a8f41",
   "metadata": {},
   "source": [
    "Use `torch.argmax` to get the ID of the predicted token from the logits. Hint- you may have to set `axis=` argument on argmax to return the right value. Then use the tokenizer's decode method to return this as a string. Hint you may have to convert the pytorch tensor to a python list using `.tolist()` and also only put a single input into the decode function "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b8e45b7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "075d834e",
   "metadata": {},
   "source": [
    "**Question:** What word does the model predict?\n",
    "\n",
    "**Answer:** to be completed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2d70dc3",
   "metadata": {},
   "source": [
    "# Exercise 2 - Fine-tuning BERT for NLI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f50d44e0",
   "metadata": {},
   "source": [
    "## Exercise 2.1 - Loading NLI Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f794749",
   "metadata": {},
   "source": [
    "We can load the NLI dataset from Huggingface datasets library. This might take a while (313MB download)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0fc3dc79",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset glue (/Users/user/.cache/huggingface/datasets/glue/mnli/1.0.0/dacbe3125aa31d7f70367a07a8a9e72a5a0bfeb5fc42e75c9db75b96da6053ad)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "06b20891735e41fab5532e87c4b7ad16",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "dataset = load_dataset(\"glue\", \"mnli\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18f7a5b6",
   "metadata": {},
   "source": [
    "We can see the example of the training data: the dataset constains a list of instances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "80f766bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'premise': 'Conceptually cream skimming has two basic dimensions - product and geography.',\n",
       " 'hypothesis': 'Product and geography are what make cream skimming work. ',\n",
       " 'label': 1,\n",
       " 'idx': 0}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[\"train\"][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c3d6261",
   "metadata": {},
   "source": [
    "**Question:** What do each of the fields in each dataset instance correspond to?\n",
    "\n",
    "**Answer:** To be completed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea1344f3",
   "metadata": {},
   "source": [
    "For use in the model, we will have to tokenize the instances from the dataset. Use the tokenizer to tokenize the instance, putting both premise and hypothesis into a single list of IDs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9778fbae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['conceptual', '##ly', 'cream', 'ski', '##mming', 'has', 'two', 'basic', 'dimensions', '-', 'product', 'and', 'geography', '.', 'product', 'and', 'geography', 'are', 'what', 'make', 'cream', 'ski', '##mming', 'work', '.']\n",
      "[101, 17158, 2135, 6949, 8301, 25057, 2038, 2048, 3937, 9646, 1011, 4031, 1998, 10505, 1012, 102, 4031, 1998, 10505, 2024, 2054, 2191, 6949, 8301, 25057, 2147, 1012, 102]\n"
     ]
    }
   ],
   "source": [
    "print(tok.tokenize(dataset['train'][0]['premise'],dataset['train'][0]['hypothesis']))\n",
    "print(tok.encode(dataset['train'][0]['premise'],dataset['train'][0]['hypothesis']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54b81256",
   "metadata": {},
   "source": [
    "**Question:** How many separator tokens were inserted by the tokeninzer? Why?\n",
    "\n",
    "**Answer:** To be completed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90d8d1a7",
   "metadata": {},
   "source": [
    "## Exercise 2.2 - Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b8ba8bf",
   "metadata": {},
   "source": [
    "We will make a tokenizer function to tokenize all instances in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f6d135c0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def tokenize_function(data):\n",
    "    # What is the type of data?\n",
    "    # Keys of the returned dictionary will be added to the dataset as columns\n",
    "    # To be completed\n",
    "    return tok(...., padding=\"max_length\", truncation=True)\n",
    "\n",
    "tokenized_datasets = dataset.map(tokenize_function, batched=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "210d90d6",
   "metadata": {},
   "source": [
    "From this, we will use the tokenizer to make a small train and validation dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f511e821",
   "metadata": {},
   "outputs": [],
   "source": [
    "small_train_dataset = tokenized_datasets[\"train\"].shuffle(seed=42).select(range(1000))\n",
    "small_eval_dataset = tokenized_datasets[\"test\"].shuffle(seed=42).select(range(100))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36e13d7e",
   "metadata": {},
   "source": [
    "We want to perform sequence classification with the BERT model. We can use the `AutoModelForSequenceClassification` class to instantiate a `bert-base-uncased` model. Remember you will have to import this class like we did the previous time. Call the model `cls_model` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7df270e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To be completed\n",
    "cls_model = ..... "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "515bc5d7",
   "metadata": {},
   "source": [
    "## Exercise 2.3 - Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "050c2e9c",
   "metadata": {},
   "source": [
    "**Question:** Make a prediction with the cls_model on the first instance of the dataset.  You will notice a paramter in the model outputs called `loss`. If you only provide input_ids, what is the loss value? If you provide both `input_ids` and `labels`, it might differ. What does this loss correspond to?\n",
    "\n",
    "**Answer:** To be completed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "540966fc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "80b202ba",
   "metadata": {},
   "source": [
    "## Exercise 2.4 \n",
    "Follow the HuggingFace tutorial to train the model for NLI (note, you can skip loading model and preparing data). Only follow the PyTorch tutorial. Do not follow the Keras/Tensorflow tutorials. \n",
    "\n",
    "Report the accuracy on the 100 validation instances\n",
    "\n",
    "* https://huggingface.co/docs/transformers/training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ee2d526d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments, Trainer\n",
    "import evaluate\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ecb1ce0",
   "metadata": {},
   "source": [
    "# Exercise 3* - Sequence to Sequence QA\n",
    "* This will be covered in depth on Tuesday and Wednesday.\n",
    "* If you finish early. Try using the example code from HuggingFace to fine-tune a T5 transformer for question answering with SQuAD.\n",
    "* Example code: https://github.com/huggingface/transformers/blob/v4.30.2/examples/pytorch/question-answering/trainer_seq2seq_qa.py"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Summer School",
   "language": "python",
   "name": "summerschool"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
