{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6e8f2134",
   "metadata": {},
   "source": [
    "# Knowledge Intensive NLP Summer School\n",
    "\n",
    "##Â Notebook 2\n",
    "\n",
    "The goals of this notebook are:\n",
    "\n",
    "* Use BERT-based DPR model for retrieving passages\n",
    "* Train a T5 model for predicting which page contains the answer\n",
    "\n",
    "\n",
    "## Resources\n",
    "You can find help for the HuggingFace library from their website: \n",
    "\n",
    "* BERT https://huggingface.co/docs/transformers/model_doc/bert\n",
    "* T5 https://huggingface.co/docs/transformers/model_doc/t5\n",
    "* Datasets https://huggingface.co/docs/datasets/index\n",
    "\n",
    "## Tutorial\n",
    "\n",
    "This notebook is based on the following tutorials:\n",
    "\n",
    "* BERT https://github.com/huggingface/transformers/tree/main/examples/pytorch/text-classification\n",
    "* Fine-tuning https://huggingface.co/docs/transformers/training\n",
    "* Language Generation https://ai.googleblog.com/2020/02/exploring-transfer-learning-with-t5.html\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fe9094b",
   "metadata": {},
   "source": [
    "## Exercise 1 - DPR for SQuAD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "803263a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "34480fb3adc243d4997653bc01b03819",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script:   0%|          | 0.00/5.27k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "023a5e161e8a400aa4d4cfa27afadafe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading metadata:   0%|          | 0.00/2.36k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e2627e49365c4be7b7489ebf9a69600d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading readme:   0%|          | 0.00/7.67k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset squad/plain_text to /Users/user/.cache/huggingface/datasets/squad/plain_text/1.0.0/d6ec3ceb99ca480ce37cdd35555d6cb2511d223b9150cce08a837ef62ffea453...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6d6ba5b64cd74327bdd53f014741f89a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data files:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bb02643f5d7f4c05852bf211a2df3fb0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/8.12M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bfbcffe858e64a0ab125531389084e9f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/1.05M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "45cc9a2cfc5e4e6c97860ced0ba245ef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting data files:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/87599 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split:   0%|          | 0/10570 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset squad downloaded and prepared to /Users/user/.cache/huggingface/datasets/squad/plain_text/1.0.0/d6ec3ceb99ca480ce37cdd35555d6cb2511d223b9150cce08a837ef62ffea453. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ee068ba692d542acb51911948d4966f6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Download the SQuAD dataset from huggingface hub (approx 30MB)\n",
    "from datasets import load_dataset\n",
    "dataset = load_dataset(\"squad\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d8c46ac",
   "metadata": {},
   "source": [
    "Inspect the first instance of the dataset. Today we will use the Wikipedia page titles, the context and the question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "56c07d43",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': '5733be284776f41900661182',\n",
       " 'title': 'University_of_Notre_Dame',\n",
       " 'context': 'Architecturally, the school has a Catholic character. Atop the Main Building\\'s gold dome is a golden statue of the Virgin Mary. Immediately in front of the Main Building and facing it, is a copper statue of Christ with arms upraised with the legend \"Venite Ad Me Omnes\". Next to the Main Building is the Basilica of the Sacred Heart. Immediately behind the basilica is the Grotto, a Marian place of prayer and reflection. It is a replica of the grotto at Lourdes, France where the Virgin Mary reputedly appeared to Saint Bernadette Soubirous in 1858. At the end of the main drive (and in a direct line that connects through 3 statues and the Gold Dome), is a simple, modern stone statue of Mary.',\n",
       " 'question': 'To whom did the Virgin Mary allegedly appear in 1858 in Lourdes France?',\n",
       " 'answers': {'text': ['Saint Bernadette Soubirous'], 'answer_start': [515]}}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[\"train\"][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60a0d124",
   "metadata": {},
   "source": [
    "Use a set to create a collection of all unique context paragraphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ed13b301",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_documents = set()\n",
    "unique_questions = set()\n",
    "\n",
    "for item in list(dataset[\"train\"]) + list(dataset[\"validation\"]):\n",
    "    unique_documents.add(item['context'])\n",
    "    unique_questions.add(item['question'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e999946f",
   "metadata": {},
   "source": [
    "This is the set of documents and queries we have to work with.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "39487323",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20958\n"
     ]
    }
   ],
   "source": [
    "print(len(uniques))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56b0d626",
   "metadata": {},
   "source": [
    "## Exercise 1.1 - Dense Embedding\n",
    "\n",
    "Use the HuggingFace pre-trained DPR enoder(s) to generate embeddings of the questions and documents. Save these in an appropriate data structure\n",
    "\n",
    "NB: there are two models a question encoder and a context encoder.\n",
    "\n",
    "* Introduction / Model pages: \n",
    " * https://huggingface.co/facebook/dpr-question_encoder-single-nq-base\n",
    " * https://huggingface.co/facebook/dpr-ctx_encoder-single-nq-base\n",
    " \n",
    "* Detailed documentation: https://huggingface.co/docs/transformers/model_doc/dpr\n",
    "\n",
    "Hints:\n",
    "* Using the `pooler_output` will yield a single embedding for the passage. \n",
    "* You may have to limit the max length of the string to 100 tokens with the tokenizer.\n",
    "* Embedding all passages will take some time. If slow, use a subset of passages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10c8b877",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "cd96a2c7",
   "metadata": {},
   "source": [
    "## Exercise 1.2 - Dense Retrieval using Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3457876",
   "metadata": {},
   "source": [
    "* Sample a question from the SQuAD dataset\n",
    "* Using the embeddings compute the similarity between the passage and all documents.  (Hint: This is an dot-product operation of the document vector and query vector)\n",
    "* Provide a list of the top 10 most likely candidate documents (hint Torch has a topk function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c541aed5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "257ecde7",
   "metadata": {},
   "source": [
    "## Exercise 1.3 - Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8797955",
   "metadata": {},
   "source": [
    "* For the entire validation set (or a suitably large subset that will run on your laptop), perform retrieval and return the top-10 pages. \n",
    "* How can you speed this up?\n",
    "* Compute the Recall@10 of the predicted pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fecd69a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "cb1e4ece",
   "metadata": {},
   "source": [
    "##Â Exercise 1.4 - Training / Fine-tuning\n",
    "\n",
    "* For a given question, sample 10 \"negative\" contexts from the dataset\n",
    "* Estimate the loss of prediction with the noise contrastive formulation (see the DPR paper eqn 2) https://arxiv.org/abs/2004.04906\n",
    "* Using pytorch-style training loop, fine-tune the model on a sample of instances \n",
    "* See this tutorial for more info: https://pytorch.org/tutorials/beginner/introyt/trainingyt.html#the-training-loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4658691d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "64f575de",
   "metadata": {},
   "source": [
    "# Exercise 2 - Sequence to Sequence Formulation\n",
    "\n",
    "For simplicity, these experiments should use the t5-small model. Training and inference might be slow. \n",
    "\n",
    "* Model page: https://huggingface.co/t5-small\n",
    "* Detailed information: https://huggingface.co/docs/transformers/model_doc/t5\n",
    "\n",
    "**This exercise will require the sentencepiece library. If not installed, pip install sentencepiece. This may require restart of the jupyter runtime.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c6b368d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting sentencepiece\n",
      "  Downloading sentencepiece-0.1.99-cp38-cp38-macosx_10_9_x86_64.whl (1.2 MB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31mââââââââââââââââââââââââââââââââââââââââ\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m0:01\u001b[0m:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: sentencepiece\n",
      "Successfully installed sentencepiece-0.1.99\n"
     ]
    }
   ],
   "source": [
    "!pip install sentencepiece"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1d3d8ef",
   "metadata": {},
   "source": [
    "##Â Exercise 2.1 Download T5 model and make simple inferences\n",
    "This t5-small model is approx 225MB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "00eccc3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "\n",
    "tokenizer = T5Tokenizer.from_pretrained(\"t5-small\")\n",
    "model = T5ForConditionalGeneration.from_pretrained(\"t5-small\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ca4ef881",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[    0, 11068, 15793,    10,  3488,   229,  7332,    58,     1]])\n",
      "['<pad> Zusammenfassen: Wo ist Rome?</s>']\n"
     ]
    }
   ],
   "source": [
    "input_ids = tokenizer(\"Summarize: Where is Rome?\", return_tensors=\"pt\").input_ids\n",
    "generated_tok = model.generate(input_ids=input_ids)\n",
    "generated_str = tokenizer.batch_decode(generated_tok.tolist())\n",
    "\n",
    "print(generated_tok)\n",
    "print(generated_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ad630f1",
   "metadata": {},
   "source": [
    "Try some other strings. What do you notice about the behaviour of the model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c79d10cf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e7c17f6d",
   "metadata": {},
   "source": [
    "## Exercise 2.2 Estimate the loss of a prediction made by the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d47cd452",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = tokenizer(\"Summarize: Where is Rome?\", return_tensors=\"pt\").input_ids\n",
    "target_ids = tokenizer(\"Italy\", return_tensors=\"pt\").input_ids\n",
    "\n",
    "output = model(input_ids=input_ids,labels=target_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02a63870",
   "metadata": {},
   "source": [
    "If we make a prediction with the model without calling generate, we see the loss, and also the tokens that the model predicted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a131ca81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 2, 32128])\n",
      "tensor(8.5941, grad_fn=<NllLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "print(output.logits.shape)\n",
    "print(output.loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d721a397",
   "metadata": {},
   "source": [
    "Estimate the average loss of predicting the Wikipedia title of pages for questions in the Squad dataset using the T5 model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52d0ddcd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a4d67dae",
   "metadata": {},
   "source": [
    "## Exercise 2.3 Training T5 model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66b038a1",
   "metadata": {},
   "source": [
    "Implement a training loop in Pytorch, following ex 1.4 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d66bfade",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1d376a86",
   "metadata": {},
   "source": [
    "##Â Exercise 2.4* Repeat Ex2.3 using the HuggingFace trainer (see Lab 1, Ex2.4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "065d7146",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Summer School",
   "language": "python",
   "name": "summerschool"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
